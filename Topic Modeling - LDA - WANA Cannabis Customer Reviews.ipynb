{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db42e01",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, stem_text\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9209a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\"en\" version of spacy doesnt work, hence we need to download this\n",
    "from spacy.cli.download import download\n",
    "download(model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a10325",
   "metadata": {},
   "source": [
    "## Loading Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the directory\n",
    "directory = 'C:/Users/Owner/Documents/Cannabis Landscape/iheartjane'\n",
    "\n",
    "# create an empty list to hold the dataframe\n",
    "df_list = []\n",
    "\n",
    "# Iterate through the files\n",
    "for i in range(1, 6):\n",
    "    file = f'wana{i}.csv'\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df1 = pd.read_csv(file_path)\n",
    "    df1 = df1[\"css-f83i1e\"]\n",
    "    df_list.append(df1)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "wana_df = pd.concat(df_list)\n",
    "\n",
    "wana_df = pd.DataFrame(list(wana_df.items()), columns = [\"index\",\"wana_reviews\"])\n",
    "\n",
    "wana_df.drop(\"index\", axis=1, inplace = True)\n",
    "\n",
    "## Remove rows with Nans\n",
    "wana_df= wana_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a496a25",
   "metadata": {},
   "source": [
    "## Clean the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8221bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any punctuation and convert to lowercase\n",
    "wana_df['review_clean'] = wana_df['wana_reviews'].apply(lambda x: re.sub('[^a-zA-Z0-9 \\n\\.]', '', x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962733c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "   for sentence in sentences:\n",
    "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data_words = list(sent_to_words(wana_df['review_clean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cff4cb",
   "metadata": {},
   "source": [
    "## Build the Bigram, Trigram Models and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cde51",
   "metadata": {},
   "source": [
    "## Build LDA Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ce9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "#pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08c939",
   "metadata": {},
   "source": [
    "## Optimising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for optimal number of topics\n",
    "# Considering 1-5 topics, as the last is cut of\n",
    "num_topics = list(range(6)[1:])\n",
    "num_keywords = 5\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                             id2word=id2word,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(corpus),\n",
    "                             passes=20,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "    \n",
    "    shown_topics = lda_model.show_topics(num_topics=i, \n",
    "                                         num_words=num_keywords,\n",
    "                                         formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "    LDA_models[i] = lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derives the Jaccard similarity of two topics\n",
    "\n",
    "#Jaccard similarity:\n",
    "#- A statistic used for comparing the similarity and diversity of sample sets\n",
    "#- J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "#- Goal is low Jaccard scores for coverage of the diverse elements\n",
    "\n",
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e85d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deriving mean stabilities\n",
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1552d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The coherence function in Gensim calculates the degree of semantic similarity between the top N words in a topic, higher the coherence, better is the model\n",
    "coherences = [CoherenceModel(model=LDA_models[i], texts=data_ready, dictionary=id2word, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]\n",
    "\n",
    "print('coherences:', coherences)\n",
    "print('mean_stabilities:', mean_stabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e25c4",
   "metadata": {},
   "source": [
    "## \n",
    "From here derive the ideal number of topics roughly through the difference between the coherence and stability per number of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeaec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=10)\n",
    "ax.set_ylabel('Metric Level', fontsize=8)\n",
    "ax.set_xlabel('Number of Topics', fontsize=8)\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42bbd14",
   "metadata": {},
   "source": [
    "##\n",
    "Your ideal number of topics will maximize coherence and minimize the topic overlap based on Jaccard similarity. In this case it looks like we'd be safe choosing topic numbers around 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f1533",
   "metadata": {},
   "source": [
    "## Final Optimal LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model with the optimal number of topics\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "#pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc812f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de600820",
   "metadata": {},
   "source": [
    "## What is the Dominant topic and its percentage contribution in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19735a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=wana_df['review_clean']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db3b7b",
   "metadata": {},
   "source": [
    "## The most representative sentences for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb57d7",
   "metadata": {},
   "source": [
    "Oberservation: \n",
    "\n",
    "1. Topic 0 is about helping sleep at night i.e feeling relaxed\n",
    "2. Topic 1 is about great taste\n",
    "3. Topic 2 is about a good high and flavour\n",
    "4. Topic 3 is about Anxiety (not sure if it reduces or increases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f307f",
   "metadata": {},
   "source": [
    "## Frequency Distribution of Word Counts in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 100, color='navy')\n",
    "plt.text(100, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(100,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(100,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(100,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(100,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 100), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=12)\n",
    "plt.xticks(np.linspace(0,100,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c644df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 100, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 100), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,100,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f61862",
   "metadata": {},
   "source": [
    "Observation:\n",
    "1. The distribution is skewed to the left, it suggests that the text contains many short sentences or document\n",
    "2. Narrow range of values suggests that the text is more uniform in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57223822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918de2db",
   "metadata": {},
   "source": [
    "## Word Clouds of Top N Keywords in Each Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da25999",
   "metadata": {},
   "source": [
    "##\n",
    "We have already viewed the topic keywords for each topic, it can be visually pleasing to observe a word cloud where the size of the words corresponds to their weight. Additionally, the subsequent plots will follow the same color scheme as the one used for the topics in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d5017",
   "metadata": {},
   "source": [
    "Conclusion about the topics:\n",
    "1. Topic 0: Helps sleep and relieve pain\n",
    "2. Topic 1: Helps relax and great taste\n",
    "3. Topic 2: Good high and Flavorful\n",
    "4. Topic 3: Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346f421",
   "metadata": {},
   "source": [
    "## Word Counts of Topic Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b482e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b52e2",
   "metadata": {},
   "source": [
    "Oberservation: Similar results to the previous one\n",
    "\n",
    "1. Topic 0 is about helping sleep at night i.e feeling relaxed\n",
    "2. Topic 1 is about great taste\n",
    "3. Topic 2 is about a good high and flavour of the gummie\n",
    "4. Topic 3 is about Anxiety (not sure if it reduces or increases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6dddf1",
   "metadata": {},
   "source": [
    "## Sentence Chart Colored by Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be92c7",
   "metadata": {},
   "source": [
    "1.  Number of documents for each topic by assigning the document to the topic that has the most weight in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea035132",
   "metadata": {},
   "source": [
    "2. Number of documents for each topic by by summing up the actual weight contribution of each topic to respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c62b06a",
   "metadata": {},
   "source": [
    "## t-SNE Clustering Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac41e4a",
   "metadata": {},
   "source": [
    "Observation:\n",
    "1. We see 4 different clusters, indicating 4 main topics\n",
    "2. There are a few outliers in every cluster\n",
    "3. The orange cluster is very close to green and blue indicating a little relatedness\n",
    "4. Some words may appear in more than one cluster, indicating that they are polysemous or have multiple meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1e1d4",
   "metadata": {},
   "source": [
    "## pyLDAVis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d659f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7657e",
   "metadata": {},
   "source": [
    "## \n",
    "The LDA analysis shows that there is no overlap in the topics on the left. One the right hand side we can see the salient features related to the topics. \n",
    "However, LDA analysis doesnt help us identify the topics, hence we have have to perform word count, weighted topic reprsentation to identify the topics. Summarizing the topics that have been identified from the above analysis follows:\n",
    "1. Topic 0: Helps sleep and relieve pain\n",
    "2. Topic 1: Helps relax and great taste\n",
    "3. Topic 2: Good high and Flavorful\n",
    "4. Topic 3: Anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64401c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
